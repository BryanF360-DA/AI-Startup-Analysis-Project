{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdMyHHgJuLfr",
        "outputId": "8134105f-fd86-4bec-ac6b-53210f419b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup Complete. HTML is ready to be searched.\n",
            "Original list length: 82\n",
            "Cleaned list length: 81\n",
            "All cleaned names: ['OpenAI', 'Google', 'IBM', 'Microsoft', 'NVIDIA', 'Anthropic', 'Amazon', 'Anduril', 'MaestroQA', 'EliseAI', 'Kustomer', 'Grammarly', 'Nexthink', 'Notion', 'Flatfile', 'Greenlight Guru', 'Motive', 'Kensho Technologies', 'Machina Labs, Inc', 'Artera', 'GrayMatter Robotics', 'Klaviyo', 'VORTO', 'EDB', 'Publica by IAS', 'System1', 'PwC', 'InspiringApps', 'Smartly', 'Domino Data Lab', 'Northwestern Mutual', 'General Motors', 'Clari', 'Metropolis Technologies', 'Lily AI', 'Samsara', 'Redflag AI', 'Gradient AI', 'AMP', 'LogRocket', 'Sojern', 'Acrisure Innovation', 'STR', 'Veritone', 'LogicMonitor', 'Riskified', 'Tempus AI', '3Play Media', 'Drata', 'Shield AI', 'CrowdStrike', 'Prosodica', 'Hyperscience', 'Valtech', 'Striveworks', 'AnthologyAI', 'Blueprint Test Prep', 'Consensus Cloud Solutions', 'Exabeam', 'Ascent', 'Laudio', 'Crusoe Energy Systems', 'Smartcat', 'Cisco ThousandEyes', 'Gynger', 'Hedra', 'Coupa', 'Quantum Rise', 'Regal.ai', 'Hebbia AI', 'True Anomaly', 'Atlassian', 'Air Space Intelligence', 'Cisco Meraki', 'Dynatrace', 'Airwallex', 'The Aerospace Corporation', 'Granica', 'AKASA', 'Superblocks', 'Northslope Technologies']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# The URL of the page we want to scrape\n",
        "url = 'https://builtin.com/artificial-intelligence/ai-companies-roundup'\n",
        "\n",
        "# Send a request to download the page\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "print(\"Setup Complete. HTML is ready to be searched.\")\n",
        "\n",
        "# Find all the h3 tags on the page\n",
        "company_tags = soup.find_all('h3')\n",
        "\n",
        "# We don't need to see the raw HTML tags anymore, so we comment this out.\n",
        "# print(company_tags)\n",
        "\n",
        "# Create an empty list to store the clean names\n",
        "company_names = []\n",
        "\n",
        "# Loop through each tag in the company_tags list\n",
        "for tag in company_tags:\n",
        "    company_names.append(tag.get_text())\n",
        "\n",
        "# We don't need to see the full, uncleaned list, so we comment this out too.\n",
        "# print(company_names)\n",
        "\n",
        "# Create a new list without the last item\n",
        "cleaned_company_names = company_names[:-1]\n",
        "\n",
        "# These final print statements are perfect! They are a clean summary.\n",
        "print(f\"Original list length: {len(company_names)}\")\n",
        "print(f\"Cleaned list length: {len(cleaned_company_names)}\")\n",
        "print(\"All cleaned names:\", cleaned_company_names)"
      ]
    }
  ]
}